\documentclass[a4paper]{article}
\usepackage{graphicx}

\title{vbsr: Variational Bayes Spike regression}
\author{Ben Logsdon}

\begin{document}

\maketitle
\section{Example 1}
We first consider the case of uncorrelated features, and a linear response, with a sparse true model:

<<>>=
library(vbsr)
library(MASS)
set.seed(1)
n <- 100
m <- 500
ntrue <- 10
e <- rnorm(n)
X <- matrix(rnorm(n*m),n,m)
tbeta <- sample(1:m,ntrue)
beta <- rep(0,m)
beta[tbeta]<- rnorm(ntrue,0,2)
y <- X%*%beta+e
res<- vbsr_net(y,X,regress="LINEAR")
res<- vbsr_net(y,X,regress="LINEAR",l0_path=seq(-50,0,length.out=50),path_length=50)
@

Next we look at the following solutions along the path of the penalty parameter for this, starting with the normally distributed test statistic:
\begin{center}
\setkeys{Gin}{width=3 in}
<<fig=TRUE>>=
plot_vbsr_beta_chi(res)
@
\end{center}
The expectation of the regression coefficients:
\begin{center}
\setkeys{Gin}{width=3 in}
<<fig=TRUE>>=
plot_vbsr_e_beta(res)
@
\end{center}
as well as the posterior probability of being non-zero:
\begin{center}
\setkeys{Gin}{width=3 in}
<<fig=TRUE>>=
plot_vbsr_beta_p(res)
@
\end{center}
the Kullback-Leibler divergence computed along the path:
\begin{center}
\setkeys{Gin}{width=3 in}
<<fig=TRUE>>=
plot_vbsr_kl(res)
@
\end{center}
and finally another diagnostic of the goodness of fit of the null features to a normal distribution along the path:
\begin{center}
\setkeys{Gin}{width=3 in}
<<fig=TRUE>>=
plot_vbsr_boxplot(res)
@
\end{center}
Let's look at what the solution at the KL minimum plus 2 standard errors looks like:
<<>>=
w_sol = which.min(abs(res$kl-res$kl_min-2*res$kl_se))
res$l0_path[w_sol];
print(sort(tbeta));
which(res$beta_p[-1,w_sol]>.99)
which(res$beta_chi[-1,w_sol]^2 > qchisq(1-0.05/1000,1))
@




\section{Example 2}
We consider the case of uncorrelated features, and a logistic response, with a sparse true model:

<<>>=
n = 200
m = 50
ntrue = 10
X <- matrix(rnorm(n*m),n,m)
tbeta <- sample(1:m,ntrue)
beta <- rep(0,m)
beta[tbeta]<- rnorm(ntrue,0,1)
pred_val <- X%*%beta
y <- rep(0,n)
for(i in 1:n){
	y[i] <- rbinom(1,1,1/(1+exp(-pred_val[i])))
}
res<- vbsr_net(y,X,regress="LOGISTIC",n_orderings=1)
@

Next we look at the following solutions along the path of the penalty parameter for this, starting with the normally distributed test statistic:
\begin{center}
\setkeys{Gin}{width=3 in}
<<fig=TRUE>>=
plot_vbsr_beta_chi(res)
@
\end{center}
The expectation of the regression coefficients:
\begin{center}
\setkeys{Gin}{width=3 in}
<<fig=TRUE>>=
plot_vbsr_e_beta(res)
@
\end{center}
as well as the posterior probability of being non-zero:
\begin{center}
\setkeys{Gin}{width=3 in}
<<fig=TRUE>>=
plot_vbsr_beta_p(res)
@
\end{center}
the Kullback-Leibler divergence computed along the path:
\begin{center}
\setkeys{Gin}{width=3 in}
<<fig=TRUE>>=
plot_vbsr_kl(res)
@
\end{center}
and finally another diagnostic of the goodness of fit of the null features to a normal distribution along the path:
\begin{center}
\setkeys{Gin}{width=3 in}
<<fig=TRUE>>=
plot_vbsr_boxplot(res)
@
\end{center}
Let's look at what the solution at the KL minimum plus 2 standard errors looks like:
<<>>=
w_sol = which.min(abs(res$kl-res$kl_min-2*res$kl_se))
res$l0_path[w_sol];
print(sort(tbeta));
which(res$beta_p[-1,w_sol]>.99)
which(res$beta_chi[-1,w_sol]^2 > qchisq(1-0.05/100,1))
@

\section{Example 3}
Now consider the case of weakly correlated features, and a linear response, with a sparse true model:

<<>>=
library(vbsr)
library(MASS)
set.seed(1)
n <- 200
m <- 200
ntrue <- 10
e <- rnorm(n)
OM <- matrix(rbinom(m^2,1,1.3/m)*rnorm(m^2),m,m);
diag(OM) <- 1;
OM <- OM%*%t(OM);
X <- scale(mvrnorm(n,rep(0,m),solve(OM)));
tbeta <- sample(1:m,ntrue)
beta <- rep(0,m)
beta[tbeta]<- rnorm(ntrue,0,2)
y <- X%*%beta+e
res<- vbsr_net(y,X,regress="LINEAR",n_orderings=100)
@

Next we look at the following solutions along the path of the penalty parameter for this, starting with the normally distributed test statistic:
\begin{center}
\setkeys{Gin}{width=3 in}
<<fig=TRUE>>=
plot_vbsr_beta_chi(res)
@
\end{center}
The expectation of the regression coefficients:
\begin{center}
\setkeys{Gin}{width=3 in}
<<fig=TRUE>>=
plot_vbsr_e_beta(res)
@
\end{center}
as well as the posterior probability of being non-zero:
\begin{center}
\setkeys{Gin}{width=3 in}
<<fig=TRUE>>=
plot_vbsr_beta_p(res)
@
\end{center}
the Kullback-Leibler divergence computed along the path:
\begin{center}
\setkeys{Gin}{width=3 in}
<<fig=TRUE>>=
plot_vbsr_kl(res)
@
\end{center}
and finally another diagnostic of the goodness of fit of the null features to a normal distribution along the path:
\begin{center}
\setkeys{Gin}{width=3 in}
<<fig=TRUE>>=
plot_vbsr_boxplot(res)
@
\end{center}
Let's look at what the solution at the end of the path looks like:
<<>>=
w_sol2=which.min(abs(res$kl-res$kl_min));
res$l0_path[w_sol2];
print(sort(tbeta));
which(res$beta_p[-1,w_sol2]>.99)
which(res$beta_chi[-1,w_sol2]^2 > qchisq(1-0.05/1000,1))
which(res$sma_chi[-res$which_excluded]^2 > qchisq(1-0.05/1000,1))
@

\end{document}


